---
layout: post
title:  "Reinforcement learning part 2"
date:   2020-05-25 18:37:37 +0100
categories: machine-learning
---

---

## Policy Gradient Methods

There are different apporaches to solving the problem given in part1. The class of method i'm going to focus on initially are Policy Gradient Methods. In particular I'm implementing REINFORCE.


<sup>__Note__: *Here I assume you know rudamentary details of how neural networks work and machine learning work. Like what they look like, that they're parameterised by lots of weights and biases. How you can compute the derivative w.r.t. these parameters using backpropagation. How the derivative of a loss function w.r.t. these parameters tells you what direction to change the parameters in order to improve the networks performance... These kinds of things.*</sup>


So one potential solution to the above problem is to take a function that takes as input the state the actor is in and then gives as output a distrubution that tells us which actions are most likely to result in fufilling the goal. Inintially this function is just a guess, it just gives random probabailities for the best action. The goal of training is to encourage the function to get better and better at suggesting the best action to take at each state. So suppose you have the luner lander environment and you record the actions that this function dictates at each state in the actors trajectory. At the end you look through each of the actions and each of the states and ask which actions resulted in positive outcomes and which resulted in negative outcomes. You then want to encourage those that resulted in success and discourage those that didn't.

The function above will be a nerual network and is known as the policy. At each state we'll give this neural policy the location of the shuttle and it'll pass that data through each of it's layers and output a vector of values. Initaily we're going to assume the set of actions are discrete so, engine on or off, rather than continuous, engine fire at 60% or maybe 65% or 65.562%. This means that the vector of values above corresponds to probabaility of which engine to fire. So given a state we compute the action probabailities and then sample from these probabailities the action we take. Becuase of this during training we don't always do the same thing, we do some things much more often but ocasionally we try stuff that the policy is mostly advising against. We then use back propigation via tensorflow to obatin the parameter change in the policy weights and biases that's going to result in the network suggesting actions that do well more than actions that do badly.

Denote a policy by $$\pi$$ and the set of parameters underlying it, a vector of weight and biases, $$\theta$$. The set of states and actions generated by taking a state, computing the policy, sampling a action, transistion to a new state and repeating is an orbit, denote such an object $$\tau$$. These look like:

$$\tau_{\theta} = [(s_{0}, a_{0}), (s_{1}, a_{1}), ..., (s_{i}, a_{i}), (s_{i+1}, a_{i+1}), ....]$$

Where $$a_{i}$$ is an action sampled from the policy probabaility distrubution $$\pi_{\theta}(s_{i})$$. If the system is deterministic then given a state and an action we can directly compute the subsequent state. So $$(s_{i}, a_{i}) \rightarrow s_{i+1}$$. If the system is deterministic then you conduct some experiement and sample the next state from the probabaility distrubution of states given by taking action $$a_{i}$$ in state $$s_{i}$$. This

When we decide we're going to encourage an action we want to know how the network changes with respect to it's parameters. Naturally we'd expect hte update rule to look something like:

$$
\theta \rightarrow \theta + A\nabla \pi_{\theta}(s_{i}, a_{i})
$$

Where $$\nabla \pi_{\theta}(s_{i}, a_{i})$$ is the derivative of the probabaility density function w.r.t. $$\theta$$ __at the chosen action for that state__. A is something positive if the $$a_{i}$$ resulted in a good outcome and negative if it didn't go well. So then the above should make $$a_{i}$$ more likely if it was good and less if it was bad.

#### The problem with the above





<!-- $$

\begin{aligned}
  & \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  & (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) & \cdots & \phi(e_1, e_n) \\
      \vdots & \ddots & \vdots \\
      \phi(e_n, e_1) & \cdots & \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{aligned}

$$ -->
